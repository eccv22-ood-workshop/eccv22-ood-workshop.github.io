<!DOCTYPE HTML>
<!--
    Helios by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
    <head>
        <title>Challenge - OOD-CV</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
        <link rel="stylesheet" href="assets/css/main.css" />
        <link rel="stylesheet" href="assets/css/user.css" />
        <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
    </head>
    <body class="no-sidebar is-preload">
        <div id="page-wrapper">

            <!-- Header -->
                <div id="header">

                    <!-- Inner -->
                    <div class="inner">
                        <header>
                            <!--<h1 class="highlight"><a href="index.html" id="logo">
                                <span>OOD</span>
                                <span style="color: inherit">-</span>
                                <span>CV</span></a></h1>-->
                            <h1>
                                <span class="highlight">O</span>
                                ut
                                <span class="highlight">O</span>
                                f
                                <span class="highlight">D</span>
                                istribution
                                Generalization
                                in
                                <span class="highlight">C</span>
                                omputer
                                <span class="highlight">V</span>
                                ision
                            </h1>
                            <h4>in conjunction with <a href="https://eccv2022.ecva.net/">ECCV 2022</a></h4>
                        </header>
                    </div>
                    <a href="https://www.springer.com/journal/11263/updates/20293358"><img class="ijcv-logo" src="images/ijcv_specail_issue_short.png"></a>

                    <!-- Nav -->
                    <nav id="nav">
                        <ul>
                                <li><a href="index.html">Home</a></li>
                                <li><a href="challenge.html">Challenge</a></li>
                                <li><a href="paper.html">Call for Papers</a></li>
                        </ul>
                    </nav>

                </div>

            <!-- Main -->
            <div class="wrapper style1">

                <div class="container">
                    <article id="main" class="special">
                        <header>
                            <h2><a href="#">Challenge Winners</a></h2>
                        </header>

                        <section>
                            <header>
                                <h2><a href="#">Image Classification</a></h2>
                            </header>
                            <div class="trend-entry d-flex"> 
                                <table class="table table-striped">
                                    <thead>
                                    <tr>
                                        <th scope="col"> Rank</th>
                                        <th scope="col"> Winners</th>
                                    </tr>
                                    </thead>
                                    <tbody> 
                                    <tr>
                                        <td>ðŸ¥‡ 1st place $2,000</td> <a href="https://arxiv.org/abs/2301.04795">[Report]</a>
                                        <td>Yilu Guo, Xingyue Shi, Weijie Chen, Shicai Yang, Di Xie, Shiliang Pu, Yueting Zhuang
                                            <br>Hikvision Research Institute, Peking University Shenzhen Graduate School, Zhejiang University
                                        </td>
                                    </tr> 
                                    <tr>
                                        <td>ðŸ¥ˆ 2nd place $1,000</td>
                                        <td>Jun Yu, Keda Lu, Hao Chang, Mohan Jing, Xiaohua Qi, Liwen Zhang, Zhihong Wei, Ye Yu, Fang Gao <a href="2nd-place-Classification.zip">[Report]</a>
                                            <br>University of Science and Technology of China 
                                        </td>
                                    </tr>
                                    <tr>
                                        <td>ðŸ¥‰ 3rd place $300</td>
                                        <td>Yuanpeng Tu, Kai Wu, Boshen Zhang, Yong Liu
                                            <br>Tongji University
                                        </td>
                                    </tr>
                                    <tr>
                                        <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4th place</td>
                                        <td>Manxi Lin, Yu Ji, Shengkai Wang <a href="4th-place-Classification.zip">[Report]</a>
                                            <br>Technical University of Denmark, University of Chinese Academy of Sciences, Sichuan University
                                        </td>
                                    </tr>
                                    </tbody>
                                </table>
                            </div>
                        </section>

                        
                        <section>
                            <header>
                                <h2><a href="#">Object Detection</a></h2>
                            </header>
                            <div class="trend-entry d-flex"> 
                                <table class="table table-striped">
                                    <thead>
                                    <tr>
                                        <th scope="col"> Rank</th>
                                        <th scope="col"> Winners</th>
                                    </tr>
                                    </thead>
                                    <tbody> 
                                    <tr>
                                        <td>ðŸ¥‡ 1st place $2,000</td><a href="https://arxiv.org/abs/2301.04796">[Report]</a>
                                        <td>Wei Zhao, Binbin Chen, Weijie Chen, Shicai Yang, Di Xie, Shiliang Pu, Yueting Zhuang
                                            <br>Hikvision Research Institute, Zhejiang University
                                        </td>
                                    </tr> 
                                    <tr>
                                        <td>ðŸ¥ˆ 2nd place $1,000</td>
                                        <td>Tae-Hyong Kim, Jahongir Yunusov, Shohruh Rakhmatov, Abdulaziz Namozov, Ki-Hwan Lee, Adnan Md Tayeb <a href="2nd-place-Detection.zip">[Report]</a>
                                            <br>Kumoh National Institute of Technology, DeltaX corp., DP World
                                        </td>
                                    </tr>
                                    <tr>
                                        <td>ðŸ¥‰ 3rd place $300</td>
                                        <td>Jun Yu, Keda Lu, Hao Chang, Mohan Jing, Xiaohua Qi, Liwen Zhang, Zhihong Wei, Ye Yu, Fang Gao <a href="3rd-place-Detection.zip">[Report]</a>
                                            <br>University of Science and Technology of China
                                        </td>
                                    </tr>
                                    <tr>
                                        <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4th place</td>
                                        <td>Zining Chen, Tianyi Wang <a href="4th-place-Detection.zip">[Report]</a>
                                            <br>Beijing University of Posts and Telecommunications
                                        </td>
                                    </tr>
                                    </tbody>
                                </table>
                            </div>
                        </section>

                        
                        <!-- <section>
                            <header>
                                <h2><a href="#">Pose Estimation</a></h2>
                            </header>
                            <div class="trend-entry d-flex"> 
                                <table class="table table-striped">
                                    <thead>
                                    <tr>
                                        <th scope="col"> Rank</th>
                                        <th scope="col"> Winners</th>
                                    </tr>
                                    </thead>
                                    <tbody> 
                                    <tr>
                                        <td>ðŸ¥‡ 1st place $2,000</td>
                                        <td>Yilu Guo, Xingyue Shi, Weijie Chen, Shicai Yang, Di Xie, Shiliang Pu, Yueting Zhuang</td>
                                    </tr> 
                                    <tr>
                                        <td>ðŸ¥ˆ 2nd place $1,000</td>
                                        <td>Jun Yu, Keda Lu, Hao Chang, Mohan Jing, Xiaohua Qi, Liwen Zhang, Zhihong Wei, Ye Yu, Fang Gao</td>
                                    </tr>
                                    <tr>
                                        <td>ðŸ¥‰ 3rd place $300</td>
                                        <td>Yuanpeng Tu, Kai Wu, Boshen Zhang, Yong Liu</td>
                                    </tr>
                                    <tr>
                                        <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4th place</td>
                                        <td>Manxi Lin, Yu Ji, Shengkai Wang</td>
                                    </tr>
                                    </tbody>
                                </table>
                            </div> -->
                        </section>
                    </article>
                </div>
            </div>

                <div class="wrapper style1">

                    <div class="container">
                        <article id="main" class="special">
                            <header>
                                <h2><a href="#">Challenge Instruction</a></h2>
                            </header>

                            <section>
                                <header>
                                    <h2><a href="#">Dataset</a></h2>
                                </header>
                                <p>
                                    The goal of our OOD-CV benchmark is do diagnose robustness of computer vision models to <b>out-of-distribution shifts</b> in the data.
                                    To achieve this goal, the benchmark consists of a <b>fixed training set</b> with 10 object categories (aeroplane, bus, car, train, boat, bicycle, motorplane, chair, dining table, sofa) 
                                    from the PASCAL VOC 2012 and ImageNet datasets. 
                                    Note that only our provided training data can be used to train a model and <b>using outside training data is not allowed</b>. 
                                    This restriction enables us to design the test set such that each test example is subject to an 
                                    out-of-distribution shift in one specific nuisance w.r.t. the training data, such as the object's 3D <b>pose</b>, <b>shape</b>, <b>texture</b>, <b>context</b>, the <b>weather</b>, and <b>occlusion</b>.
                                    <br><br>
                                    The OOD-CV challenge has three tracks. Each track will focus on one computer vision task, including <b>image classification, object detection and 3D pose estimation</b>. The winners of each track will be invited to contribute to an IJCV special issue.
                                    Moreover, the winners and runner-ups in each challenge track will share a prize pool of 10'000 USD.
                                </p>                                
                                    <p>
                                        <img src="images/overview.png" width="100%"/>
                                    </p>
                            </section>

                            <section>
                                <header>
                                    <h3>Data and Baselines</h3>
                                </header>
                                <p>
                                    The training data, validation data and code for the baseline models can be downloaded from this website: <b><a href="https://github.com/eccv22-ood-workshop/ROBIN-dataset">https://github.com/eccv22-ood-workshop/ROBIN-dataset</a></b><br><br>
                                    
                                        
                                    The final test set will not be publicly available (see details of the evaluation process below).                                   
                                    Note that the training set is fixed and <b>the use any outside data is not allowed</b>.
                                    All prize winners (i.e. winning submissions and runner-ups) will be required to either publish their code online, or 
                                    send us their code privately, such that we can verify that no outside data has been used.                                    
                                </p>
                            </section>

                            <section>
                                <header>
                                    <h3>Benchmark Scoring</h3>
                                </header>
                                <p>
                                    Our aim is to measure model robustness w.r.t. OOD nuisance factors. Therefore, the final benchmark scoring is <b>data and accuracy constrained</b>. 
									This means, that to be valid a submission must: <br>
									1) Only use the training data that 	we provide. Using outside data is not allowed.<br> 
									2) The modelâ€™s accuracy on the <u>I.I.D. test set</u> must be lower than a pre-defined threshold (which is defined by the performance of a baseline model).<br> 
									The final benchmark score is then measured as average performance on the held-out O.O.D. test set.<br><br>
									
									The <b>I.I.D. accuracy thresholds</b> are as follows:<br>
									Image Classification = <b>91.1 [top-1 accuracy]</b><br>
									Object Detection = <b>79.9 [mAP@50]</b><br>
									Pose Estimation  = <b>68.7 [Acc@pi/6]</b><br>
									Each accuracy threshold was determined by training the baseline models (see paper and Github repo) five times, followed by computing the mean performance and adding three standard deviations. 
                                </p>
                            </section>

                            <section>
                                <header>
                                    <h3>Submission</h3>
                                </header>
                                <p>
                                    You can find instructions on the submission, data and baseline models on our Github page: <b><a href="https://github.com/eccv22-ood-workshop/ROBIN-dataset">https://github.com/eccv22-ood-workshop/ROBIN-dataset</a></b>
                                </p>
                            </section>
                            
                            <section>
                                <header>
                                    <h3>Prizes</h3>
                                </header>
                                <p>
                                    The winners of each track will be invited to submit a paper to <b><a href="https://www.springer.com/journal/11263/updates/20293358">an IJCV special issue</a></b>.
                                    The top-3 submissions across all challenge tracks will share a <b>prize pool of 10'000 USD</b> which is sponsored by the Future Fund Regranting Program.
                                    All prize winners will be required to write a summary of their method and share it publicly on arXiv.<br><br>
									
									<b><a href="https://codalab.lisn.upsaclay.fr/competitions/6781">Image Classification Track</a></b><br>
									ðŸ¥‡ 1st place: $2,000<br>
									ðŸ¥ˆ 2nd place: $1,000<br>
									ðŸ¥‰ 3rd place: $300<br><br>
									<b><a href="https://codalab.lisn.upsaclay.fr/competitions/6784">Object Detection Track</a></b><br>
									ðŸ¥‡ 1st place: $2,000<br>
									ðŸ¥ˆ 2nd place: $1,000<br>
									ðŸ¥‰ 3rd place: $300<br><br>
									<b><a href="https://codalab.lisn.upsaclay.fr/competitions/6783">Pose Estimation Track</a></b><br>
									ðŸ¥‡ 1st place: $2,000<br>
									ðŸ¥ˆ 2nd place: $1,000<br>
									ðŸ¥‰ 3rd place: $300<br>
                                </p>
                            </section>
                            <h3><u>Important Dates</u></h3>
                            <div class="trend-entry d-flex"> 
                                <table class="table table-striped">
                                    <thead>
                                    <tr>
                                        <th scope="col"> Description</th>
                                        <th scope="col"> Date</th>
                                    </tr>
                                    </thead>
                                    <tbody> 
                                    <tr>
                                        <td>Training data and development kit will be released</td>
                                        <td>June 6th, 2022</td>
                                    </tr> 
                                    <tr>
                                        <td>Phase-1 starts: Public release of inital test set</td>
                                        <td>June 30th, 2022</td>
                                    </tr>
                                    <tr>
                                        <td>Phase-2: Final test set released on Codalab</td>
                                        <td>September 25th, 2022</td>
                                    </tr>
                                    <tr>
                                        <td>Submission deadline</td>
                                        <td>September 30th, 2022</td>
                                    </tr>
                                    <tr>
                                        <td>Challenge report and code deadline</td>
                                        <td>October 12th, 2022</td>
                                    </tr>
                                  <!--   <tr>
                                        <td>Poster</td>
                                        <td>10:00-11:00, June 16, 2021</td>
                                    </tr> -->
                                    </tbody>
                                </table>
                            </div>
                            <section>
                                <header>
                                    <h3>Citation</h3>
                                </header>
                                <p>
                                    If you find the dataset or challenge useful for your research, please cite the paper.
                                </p>
                                <pre style="background-color: #e2e2e2; border: 0px dotted #818181; padding: 0px;">			
                                    <span class="inner-pre" style="font-size: 18px">
                                    @article{zhao22oodcv,
                                        author  = {Bingchen Zhao and Shaozuo Yu and Wufei Ma and Mingxin Yu and Shenxiao Mei and Angtian Wang and Ju He and Alan Yuille and Adam Kortylewski},
                                        title   = {OOD-CV: A Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images},
                                        journal = {Proceedings of the European Conference on Computer Vision (ECCV)},
                                        year    = {2022}
                                    }			
                                    </span>
                                </pre>
                            </section>
							
                            <section>
                                <header>
                                    <h3>Eligibility</h3>
                                </header>
                                <p>
									To be eligible for prizes, winning teams are required to share their methods, code, and models with the organizers as well as the names and associations of each team member.									
									This workshop is sponsored by the FTX Future Fund regranting program. 
									We cannot give awards to teams on US terrorist lists or those subject to sanctions. 
									Sponsor may confirm the legality of sending prize money to winners who are residents of countries outside of the United States. 
									Winners will be announced prior to the workshop, and workshop organizers will judge the submissions. Only authors on awarded papers are winners. 
									All decisions of judges are final. The legality of accepting the prize in his or her country is the responsibility of the winners. 
									All taxes are the responsibility of the winners. Employees or current contractors of FTX and contest organizers are not eligible to win prizes. 
									Entrants must be over the age of 18. By entering the contest, entrants agree to the Terms & Conditions. 
									Entrants agree that FTX shall not be liable to entrants for any type of damages that arise out of or are related to the contest and/or the prizes. 
									By submitting an entry, entrant represents and warrants that, consistent with the terms of the Terms and Conditions: (a) the entry is entrantâ€™s original work; 
									(b) entrant owns any copyright applicable to the entry; (c) the entry does not violate, in whole or in part, any existing copyright, trademark, patent or any other intellectual property right of any other person, organization or entity; 
									(d) entrant has confirmed and is unaware of any contractual obligations entrant has which may be inconsistent with these Terms and Conditions and the rights entrant is required to have in the entry, including but not limited to any prohibitions, 
									obligations or limitations arising from any current or former employment arrangement entrant may have; (e) entrant is not disclosing the confidential, trade secret or proprietary information of any other person or entity, 
									including any obligation entrant may have in connection arising from any current or former employment, without authorization or a license; and (f) entrant has full power and all legal rights to submit an entry in full compliance with these Terms and Conditions.

                                </p>
                            </section>							
                        </article>
                    </div>

                </div>


        </div>

        <!-- Scripts -->
            <script src="assets/js/jquery.min.js"></script>
            <script src="assets/js/jquery.dropotron.min.js"></script>
            <script src="assets/js/jquery.scrolly.min.js"></script>
            <script src="assets/js/jquery.scrollex.min.js"></script>
            <script src="assets/js/browser.min.js"></script>
            <script src="assets/js/breakpoints.min.js"></script>
            <script src="assets/js/util.js"></script>
            <script src="assets/js/main.js"></script>

    </body>
</html>
